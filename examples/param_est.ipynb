{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Parameter Estimation Feature Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to instruct ProgPy users on how to use the estimate_params feature for PrognosticModels.\n",
    "\n",
    "First some background. Parameter estimation is used to tune the parameters of a general model so its behavior matches the behavior of a specific system. For example, parameters of the battery model can be tuned to configure the model to describe the behavior of a specific battery.\n",
    "\n",
    "Generally, parameter estimation is done by tuning the parameters of the model so that simulation best matches the behavior observed in some available data. In ProgPy, this is done using the prog_models.PrognosticsModel.estimate_params() method. This method takes input and output data from one or more runs, and uses scipy.optimize.minimize function to estimate the parameters of the model. For more information, refer to our Documentation [here](https://nasa.github.io/progpy/prog_models_guide.html#parameter-estimation)\n",
    "\n",
    "A few definitions:\n",
    "* __`keys`__ `(list[str])`: Parameter keys to optimize\n",
    "* __`times`__ `(list[float])`: Array of times for each run\n",
    "* __`inputs`__ `(list[InputContainer])`: Array of input containers where inputs[x] corresponds to times[x]\n",
    "* __`outputs`__ `(list[OutputContainer])`: Array of output containers where outputs[x] corresponds to times[x]\n",
    "* __`method`__ `(str, optional)`: Optimization method- see scipy.optimize.minimize for options\n",
    "* __`tol`__ `(int, optional)`: Tolerance for termination. Depending on the provided minimization method, specifying tolerance sets solver-specific options to tol\n",
    "* __`error_method`__ `(str, optional)`: Method to use in calculating error. See calc_error for options\n",
    "* __`bounds`__ `(tuple or dict, optional)`: Bounds for optimization in format ((lower1, upper1), (lower2, upper2), ...) or {key1: (lower1, upper1), key2: (lower2, upper2), ...}\n",
    "* __`options`__ `(dict, optional)`: Options passed to optimizer. See scipy.optimize.minimize for options"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1) Simple Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will show an example demonstrating the model parameter estimation feature. In this example, we will be estimating the parameters for a model from data . In general, the data will usually be collected from the physical system or from a different model (model surrogacy). \n",
    "\n",
    "First, we will import a model from the ProgPy Package. For this example we're using the simple ThrownObject model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prog_models.models import ThrownObject"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a model with a best guess for the parameters.\n",
    "\n",
    "We will use a guess that our thrower is 20 meters tall. However, given our times, inputs, and outputs, we can clearly tell this is not true! Let's see if parameter estimation can fix this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ThrownObject(thrower_height=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will collect data from the system. Let's pretend we threw the ball once, and collected position measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "inputs = [{}]*9\n",
    "outputs = [\n",
    "    {'x': 1.83},\n",
    "    {'x': 36.95},\n",
    "    {'x': 62.36},\n",
    "    {'x': 77.81},\n",
    "    {'x': 83.45},\n",
    "    {'x': 79.28},\n",
    "    {'x': 65.3},\n",
    "    {'x': 41.51},\n",
    "    {'x': 7.91},\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will define specific parameters that we want to estimate.\n",
    "\n",
    "We can pass the desired parameters to our __keys__ keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['thrower_height', 'throwing_speed']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To really see what `estimate_params()` is doing, we will print out the state before executing the estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing state before\n",
    "print('Model configuration before')\n",
    "for key in keys:\n",
    "    print(\"-\", key, m.parameters[key])\n",
    "print(' Error: ', m.calc_error(times, inputs, outputs, dt=1e-4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the error is quite high. This indicates that the parameters are not accurate\n",
    "\n",
    "Now, we will run `estimate_params()` with the data to correct these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.estimate_params(times = times, inputs = inputs, outputs = outputs, keys = keys, dt=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what the new parameters are after estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nOptimized configuration')\n",
    "for key in keys:\n",
    "    print(\"-\", key, m.parameters[key])\n",
    "print(' Error: ', m.calc_error(times, inputs, outputs, dt=1e-4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough- parameter estimation determined that the thrower's height wasn't 20m, instead was closer to 1.9m, a much more reasonable height!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2) Using Tol"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional feature of the `estimate_params()` function is the tolerance feature, or `tol`. The `tol` makes our parameter estimation function to continue optimizing until we reach a particular error.\n",
    "\n",
    "In our previous example, note that our total Error was roughly 0.5272 after the `estimate_params()` call. Now, let us see what happens to the parameters when we set a low tolerance and bounds to their respective keys!\n",
    "\n",
    "First, let us create a more complicated example! In this example, we are selecting our thrower_height to be 29, our throwing_speed to be 3.1, and our g to be 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ThrownObject()\n",
    "results = m.simulate_to_threshold(save_freq=0.5)\n",
    "# Resetting parameters to their originally incorrectly set values.\n",
    "m.parameters['thrower_height'] = 3.1\n",
    "m.parameters['throwing_speed'] = 29\n",
    "m.parameters['g'] = 10\n",
    "\n",
    "# Furthermore, changing our keys values to encompass all incorrectly set parameters\n",
    "keys = ['thrower_height', 'throwing_speed', 'g']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll set the bounds of our values. Let's say that we know our thrower_height will be some value between -15 and 15, our throwing_speed somewhere between 24 and 42, and the gravity is somewhere between -20 and 10.\n",
    "\n",
    "Note that we are calling `simulate_to_threshold()` here instead of using data that is readily available. Typically, either data has been collected by the user, or the user can utilize our Simulation features to predict how the system would change overtime! More information can be found [here](https://nasa.github.io/progpy/prog_models_guide.html#simulation)!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all our information, it's time to call our `estimate_params()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.estimate_params(times = results.times, inputs = results.inputs, outputs = results.outputs, keys = keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nOptimized configuration')\n",
    "for key in keys:\n",
    "    print(\"-\", key, m.parameters[key])\n",
    "print(' Error: ', m.calc_error(results.times, results.inputs, results.outputs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's reset our parameters to their incorrect values, and then call `estimate_params()` with a low tolerance value passed in! in this case, we are passing in a value of __1e-9__ to `tol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.parameters['thrower_height'] = 3.1\n",
    "m.parameters['throwing_speed'] = 29\n",
    "m.parameters['g'] = 10\n",
    "m.estimate_params(times = results.times, inputs = results.inputs, outputs = results.outputs, keys = keys, tol=1e-9)\n",
    "print('\\nOptimized configuration')\n",
    "for key in keys:\n",
    "    print(\"-\", key, m.parameters[key])\n",
    "print(' Error: ', m.calc_error(results.times, results.inputs, results.outputs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, if we were to set a high tolerance, such as 10, our error would consequently be very high!\n",
    "\n",
    "For more information on how the `tol` feature works, please consider scipy's `minimize()` documentation located [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our optimization function, which runs `calc_error()` as a subroutine (more information about `calc_error()` found in our Calculating Error Example).\n",
    "\n",
    "You can also adjust the metric that is used to estimate parameters by setting the error_method to a different `calc_error()` method.\n",
    "e.g., m.estimate_params([(times, inputs, outputs)], keys, dt=0.01, error_method='MAE')\n",
    "Default is Mean Squared Error (MSE)\n",
    "See calc_error method for list of options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.parameters['thrower_height'] = 3.1\n",
    "m.parameters['throwing_speed'] = 29\n",
    "m.parameters['g'] = 10\n",
    "# Using MAE, or Mean Absolute Error instead of the default Mean Squared Error.\n",
    "m.estimate_params(times = results.times, inputs = results.inputs, outputs = results.outputs, keys = keys, tol=1e-9, error_method = 'MAE')\n",
    "print('\\nOptimized configuration')\n",
    "for key in keys:\n",
    "    print(\"-\", key, m.parameters[key])\n",
    "print(' Error: ', m.calc_error(results.times, results.inputs, results.outputs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3) Handling Noise with Multiple Runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous two examples, we have demonstrated how to use `estimate_params()` using a clearly defined ThrownObject model. However, unlike most models, we assumed that there would be 0 noise to our system!\n",
    "\n",
    "In this example, we'll show how `estimate_params()` may not necessarily produce optimal results when handling a system with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ThrownObject(process_noise = 1)\n",
    "results = m.simulate_to_threshold(save_freq=0.5)\n",
    "# Resetting parameters to their originally incorrectly set values.\n",
    "m.parameters['thrower_height'] = 3.1\n",
    "m.parameters['throwing_speed'] = 29\n",
    "m.parameters['g'] = 10\n",
    "\n",
    "# Furthermore, changing our keys values to encompass all incorrectly set parameters\n",
    "keys = ['thrower_height', 'throwing_speed', 'g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.estimate_params(times = results.times, inputs = results.inputs, outputs = results.outputs, keys = keys)\n",
    "print('\\nOptimized configuration')\n",
    "for key in keys:\n",
    "    print(\"-\", key, m.parameters[key])\n",
    "print(' Error: ', m.calc_error(results.times, results.inputs, results.outputs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is to have a good estimation of the error, we should be manually measuring the Absolute Mean Error rather than using calc_error().\n",
    "\n",
    "The reason being is simple! calc_error() is simulating the error between teh simulated and observed data, however, observed and simulated data in this case are being generated from a model that has noise! In other words, we are comparing the difference of noise to noise, which can lead to inconsistent results!\n",
    "\n",
    "Let's create a helper function to calculate the Absolute Mean Error between our original and estimated parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the Absolute Mean Error (AME) of the model.\n",
    "def AME(m, keys):\n",
    "    error = 0\n",
    "    true_Values = ThrownObject() # Creating a new model with the original parameters to compare to the model with noise.\n",
    "    for key in keys:\n",
    "        error += abs(m.parameters[key] - true_Values.parameters[key])\n",
    "    return error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error that was outputted is an error that changes for every simulated model. In otherwords,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "while count <= 10:\n",
    "    m = ThrownObject(process_noise = 1)\n",
    "    results = m.simulate_to_threshold(save_freq=0.5)\n",
    "    # Resetting parameters to their originally incorrectly set values.\n",
    "    m.parameters['thrower_height'] = 3.1\n",
    "    m.parameters['throwing_speed'] = 29\n",
    "    m.parameters['g'] = 10\n",
    "\n",
    "    m.estimate_params(times = results.times, inputs = results.inputs, outputs = results.outputs, keys = keys)\n",
    "    error = AME(m, ['thrower_height', 'throwing_speed', 'g'])\n",
    "    print(f'Estimate Call Number {count} - AME Error {error}')\n",
    "    count += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, `estimate_params()` applies the `calc_error()` method to each run independently (e.g., Run 0 = (times[]))\n",
    "\n",
    "`estimate_params()` creates a structure of 'runs' by taking each index of times, inputs, and outputs and placing them into a tuple.\n",
    "\n",
    "                `runs = [(t, u, z) for t, u, z in zip(times, inputs, outputs)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = [[], [], []]\n",
    "count = 1\n",
    "while count <= 100:\n",
    "    m = ThrownObject(process_noise = 1)\n",
    "    results = m.simulate_to_threshold(save_freq=0.5)\n",
    "    # Resetting parameters to their originally incorrectly set values.\n",
    "    m.parameters['thrower_height'] = 3.1\n",
    "    m.parameters['throwing_speed'] = 29\n",
    "    m.parameters['g'] = 10\n",
    "    \n",
    "    runs[0].append(results.times)\n",
    "    runs[1].append(results.inputs)\n",
    "    runs[2].append(results.outputs)\n",
    "    count+=1\n",
    "\n",
    "print(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.estimate_params(times = runs[0], inputs = runs[1], outputs = runs[2], keys = keys)\n",
    "print('\\nOptimized configuration')\n",
    "for key in keys:\n",
    "    print(\"-\", key, m.parameters[key])\n",
    "error = AME(m, ['thrower_height', 'throwing_speed', 'g'])\n",
    "print('AME Error: ', error)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that by creating multiple runs, we are able to produce a lower AME Error than before! This is because we are able to simulate the noise multiple times, which in turn, allows our `estimate_params()` to produce a more accurate result since it is given more values to work with!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
